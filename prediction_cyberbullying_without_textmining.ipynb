{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Megha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Megha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Megha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "import string \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "import pandas as pd\n",
    "df1 = pd.read_csv(\"cleanprojectdataset.csv\")\n",
    "df1['Text Label'] = np.where(df1['Text Label']=='Bullying',1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Tweet  Text Label\n",
      "0     .omg why are poc wearing fugly blue contacts s...           0\n",
      "1     .Sorry but most of the runners popular right n...           0\n",
      "2     .those jeans are hideous, and I?m afraid he?s ...           0\n",
      "3     .I had to dress up for a presentation in class...           0\n",
      "4     .Am I the only one who thinks justin bieber is...           0\n",
      "...                                                 ...         ...\n",
      "1060  No we are not, But you are a race baiting libt...           1\n",
      "1061  you wont get anyone for this challenge., after...           1\n",
      "1062  I will follow you if you are not a libtard,Mus...           1\n",
      "1063  michaelianblack Ur a child, an ostrich w/ your...           1\n",
      "1064  FoxNews. not to all the ppl I know that live t...           1\n",
      "\n",
      "[1065 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize words and labels into lists\n",
    "\n",
    "Tweet = []\n",
    "Labels = []\n",
    "\n",
    "for row in df1['Tweet']:\n",
    "    #tokenize words\n",
    "    words = word_tokenize(row)\n",
    "    #remove punctuations\n",
    "    clean_words = [word.lower() for word in words if word not in set(string.punctuation)]\n",
    "    #remove stop words\n",
    "    english_stops = set(stopwords.words('english'))\n",
    "    characters_to_remove = [\"''\",'``',\"rt\",\"https\",\"’\",\"“\",\"”\",\"\\u200b\",\"--\",\"n't\",\"'s\",\"...\",\"//t.c\" ]\n",
    "    clean_words = [word for word in clean_words if word not in english_stops]\n",
    "    clean_words = [word for word in clean_words if word not in set(characters_to_remove)]\n",
    "    #Lematise words\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemma_list = [wordnet_lemmatizer.lemmatize(word) for word in clean_words]\n",
    "    Tweet.append(lemma_list)\n",
    "\n",
    "    for row in df1[\"Text Label\"]:\n",
    "        Labels.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine them to create bag of words\n",
    "combined = zip(Tweet, Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create bag of words and dictionary object\n",
    "def bag_of_words(words):\n",
    "    return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Key, Value Pair into new list for modeling\n",
    "Final_Data = []\n",
    "for r, v in combined:\n",
    "    bag_of_words(r)\n",
    "    Final_Data.append((bag_of_words(r),v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1065\n"
     ]
    }
   ],
   "source": [
    "#random shuffle\n",
    "import random\n",
    "random.shuffle(Final_Data)\n",
    "print(len(Final_Data))\n",
    "#print(Final_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df1[\"Tweet\"], \n",
    "                                                    df1[\"Text Label\"], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.093896713615024"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def func_one():\n",
    "    \n",
    "    return len(df1[df1['Text Label']==1])/len(df1['Text Label'])*100\n",
    "\n",
    "func_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'healtheworldin5words'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def func_two():\n",
    "    \n",
    "    # List all tokens and their counts as a dictionary:\n",
    "    vocabulary = CountVectorizer().fit(X_train).vocabulary_\n",
    "    \n",
    "    # You want only the keys, i.e, the words:\n",
    "    vocabulary = [x for x in vocabulary.keys()]\n",
    "    \n",
    "    # Store the lengths in a separate list:\n",
    "    len_vocabulary = [len(x) for x in vocabulary]\n",
    "    \n",
    "    # Use the index of the longest token:\n",
    "    return vocabulary[np.argmax(len_vocabulary)]\n",
    "\n",
    "func_two()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6522959183673469"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def func_three():\n",
    "    \n",
    "    cv = CountVectorizer().fit(X_train)\n",
    "    \n",
    "    # Transform both X_train and X_test with the same CV object:\n",
    "    X_train_cv = cv.transform(X_train)\n",
    "    X_test_cv = cv.transform(X_test)\n",
    "    \n",
    "    # Classifier for prediction:\n",
    "    clf = MultinomialNB(alpha=0.1)\n",
    "    clf.fit(X_train_cv, y_train)\n",
    "    preds_test = clf.predict(X_test_cv)\n",
    "    \n",
    "    return roc_auc_score(y_test, preds_test)\n",
    "\n",
    "func_three()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
