{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Megha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Megha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Megha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "import string \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "import pandas as pd\n",
    "df1 = pd.read_csv(\"cleanprojectdataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Tweet    Text Label\n",
      "0     .omg why are poc wearing fugly blue contacts s...  Non-Bullying\n",
      "1     .Sorry but most of the runners popular right n...  Non-Bullying\n",
      "2     .those jeans are hideous, and I?m afraid he?s ...  Non-Bullying\n",
      "3     .I had to dress up for a presentation in class...  Non-Bullying\n",
      "4     .Am I the only one who thinks justin bieber is...  Non-Bullying\n",
      "...                                                 ...           ...\n",
      "1060  No we are not, But you are a race baiting libt...      Bullying\n",
      "1061  you wont get anyone for this challenge., after...      Bullying\n",
      "1062  I will follow you if you are not a libtard,Mus...      Bullying\n",
      "1063  michaelianblack Ur a child, an ostrich w/ your...      Bullying\n",
      "1064  FoxNews. not to all the ppl I know that live t...      Bullying\n",
      "\n",
      "[1065 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize words and labels into lists\n",
    "\n",
    "Tweet = []\n",
    "Labels = []\n",
    "\n",
    "for row in df1['Tweet']:\n",
    "    #tokenize words\n",
    "    words = word_tokenize(row)\n",
    "    #remove punctuations\n",
    "    clean_words = [word.lower() for word in words if word not in set(string.punctuation)]\n",
    "    #remove stop words\n",
    "    english_stops = set(stopwords.words('english'))\n",
    "    characters_to_remove = [\"''\",'``',\"rt\",\"https\",\"’\",\"“\",\"”\",\"\\u200b\",\"--\",\"n't\",\"'s\",\"...\",\"//t.c\" ]\n",
    "    clean_words = [word for word in clean_words if word not in english_stops]\n",
    "    clean_words = [word for word in clean_words if word not in set(characters_to_remove)]\n",
    "    #Lematise words\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemma_list = [wordnet_lemmatizer.lemmatize(word) for word in clean_words]\n",
    "    Tweet.append(lemma_list)\n",
    "\n",
    "    for row in df1[\"Text Label\"]:\n",
    "        Labels.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine them to create bag of words\n",
    "combined = zip(Tweet, Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create bag of words and dictionary object\n",
    "def bag_of_words(words):\n",
    "    return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Key, Value Pair into new list for modeling\n",
    "Final_Data = []\n",
    "for r, v in combined:\n",
    "    bag_of_words(r)\n",
    "    Final_Data.append((bag_of_words(r),v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1065\n"
     ]
    }
   ],
   "source": [
    "+#random shuffle\n",
    "import random\n",
    "random.shuffle(Final_Data)\n",
    "print(len(Final_Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                     low = True           Bullyi : Non-Bu =      9.2 : 1.0\n",
      "               worthless = True           Bullyi : Non-Bu =      7.9 : 1.0\n",
      "                 someone = True           Non-Bu : Bullyi =      7.9 : 1.0\n",
      "                      iq = True           Bullyi : Non-Bu =      6.6 : 1.0\n",
      "                   lying = True           Bullyi : Non-Bu =      5.4 : 1.0\n",
      "                      ur = True           Bullyi : Non-Bu =      5.4 : 1.0\n",
      "                 libtard = True           Bullyi : Non-Bu =      5.3 : 1.0\n",
      "                    cunt = True           Bullyi : Non-Bu =      4.8 : 1.0\n",
      "                  person = True           Non-Bu : Bullyi =      4.8 : 1.0\n",
      "                   piece = True           Bullyi : Non-Bu =      4.7 : 1.0\n",
      "0.6477987421383647\n",
      "bullying precision: 0.5403726708074534\n",
      "bullying recall: 0.696\n",
      "bullying F-measure: 0.6083916083916083\n",
      "not-bullying precision: 0.7579617834394905\n",
      "not-bullying recall: 0.616580310880829\n",
      "not-bullying F-measure: 0.6799999999999999\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = Final_Data[0:747], Final_Data[747:]\n",
    "\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
    "nb_classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nb_classifier.show_most_informative_features(10)\n",
    "\n",
    "from nltk.classify.util import accuracy\n",
    "print(accuracy(nb_classifier, test_set))\n",
    "\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "    \n",
    "for i, (Final_Data, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = nb_classifier.classify(Final_Data)\n",
    "    testsets[observed].add(i)\n",
    "    \n",
    "print('bullying precision:', precision(refsets['Bullying'], testsets['Bullying']))\n",
    "print('bullying recall:', recall(refsets['Bullying'], testsets['Bullying']))\n",
    "print('bullying F-measure:', f_measure(refsets['Bullying'], testsets['Bullying']))\n",
    "print('not-bullying precision:', precision(refsets['Non-Bullying'], testsets['Non-Bullying']))\n",
    "print('not-bullying recall:', recall(refsets['Non-Bullying'], testsets['Non-Bullying']))\n",
    "print('not-bullying F-measure:', f_measure(refsets['Non-Bullying'], testsets['Non-Bullying']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
